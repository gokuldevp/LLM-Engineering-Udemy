{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 – Web Summarizer Using Ollama (Local Open-Source Model)\n",
    "\n",
    "This notebook is a solution to the Day 2 homework assignment:\n",
    "\n",
    "> Upgrade the Day 1 project to summarize a webpage to use an open-source model running locally via **Ollama** rather than **OpenAI's cloud APIs**.\n",
    "\n",
    "We reuse the same structure from Day 1:\n",
    "\n",
    "1. A function to fetch website contents\n",
    "2. System and user prompts\n",
    "3. A `messages_for(...)` helper\n",
    "4. A `summarize(url)` function\n",
    "5. A `display_summary(url)` helper that renders markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Ollama client setup\n",
    "\n",
    "We will call Ollama through its **OpenAI-compatible** HTTP endpoint:\n",
    "\n",
    "- Base URL: `http://localhost:11434/v1/`\n",
    "- Model: `llama3.2` (or `llama3.2:1b` for smaller machines)\n",
    "\n",
    "Note: Ollama does not require a real API key, but the `openai` client expects something, so we pass a dummy string like `\"ollama\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scraper import fetch_website_contents\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "# Ollama OpenAI-compatible endpoint\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1/\"\n",
    "\n",
    "# Choose the Ollama model you pulled, e.g.:\n",
    "#   ollama pull llama3.2\n",
    "# or for smaller machines:\n",
    "#   ollama pull llama3.2:1b\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "\n",
    "# Dummy key is fine – Ollama ignores it but the client requires one\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompts and message builder (same concept as Day 1)\n",
    "\n",
    "We keep the same structure as Day 1, just changing the tone slightly if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a smart assistant that analyzes the contents of a website\n",
    "and provides a short, clear, slightly humorous summary, ignoring text that is purely navigation.\n",
    "Respond in markdown. Do not wrap the markdown in a code block – respond just with the markdown.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Here are the contents of a website.\n",
    "Provide a short summary of this website.\n",
    "If it includes news or announcements, then summarize these too.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def messages_for(website: str):\n",
    "    \"\"\"Build messages in the same format as Day 1, but for Ollama.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + website},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summarization and display helpers\n",
    "\n",
    "These follow the same pattern as Day 1, but call `ollama.chat.completions.create` instead of the OpenAI cloud endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url: str) -> str:\n",
    "    \"\"\"Fetch the website contents and ask the local Ollama model to summarize it.\"\"\"\n",
    "    website = fetch_website_contents(url)\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=messages_for(website),\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def display_summary(url: str):\n",
    "    \"\"\"Convenience helper to show the summary nicely in the notebook output.\"\"\"\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example usage\n",
    "\n",
    "Once you have Ollama running and have pulled the model (for example `llama3.2`), you can test the summarizer below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try summarizing a site of your choice\n",
    "# Example:\n",
    "# display_summary(\"https://edwarddonner.com\")\n",
    "\n",
    "display_summary(\"https://edwarddonner.com\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
